{
  "framework_name": "Data Science Methodology - Final Validation Checklist",
  "version": "1.0",
  "description": "Complete final JSON checklist to validate data preparation",
  "total_items": 88,
  "created_date": "2026-01-28",
  
  "checklist_categories": [
    {
      "category_id": "BASIC",
      "category_name": "Basic Preparation (Data Structure)",
      "severity": "CRITICAL",
      "description": "Fundamental issues that affect the entire analysis",
      "order": 1,
      "items": [
        {
          "id": "CHK-001",
          "problem_ref": "P01",
          "task": "Missing values handled",
          "description": "All missing values have been identified, quantified and resolved (removed, imputed, or marked as flags)",
          "checked": false,
          "acceptance_criteria": [
            "df.isnull().sum().sum() == 0 or documented per column",
            "Imputation method recorded",
            "Original missing rate documented"
          ],
          "effort": "Medium",
          "impact": "High"
        },
        {
          "id": "CHK-002",
          "problem_ref": "P02",
          "task": "Duplicates removed or justified",
          "description": "Duplicate rows/records have been analyzed and decided (remove, aggregate, keep)",
          "checked": false,
          "acceptance_criteria": [
            "Exact duplicates removed",
            "Partial duplicates analyzed in context",
            "Decision recorded: keep, remove, or aggregate"
          ],
          "effort": "Medium",
          "impact": "High"
        },
        {
          "id": "CHK-003",
          "problem_ref": "P03",
          "task": "Outliers analyzed and decided",
          "description": "Outliers have been identified with multiple methods and action taken (correct, remove, capping, or keep)",
          "checked": false,
          "acceptance_criteria": [
            "Detection method documented (IQR, Z-score, Isolation Forest, etc)",
            "Outliers validated against business context",
            "Action taken documented"
          ],
          "effort": "Medium",
          "impact": "Medium"
        },
        {
          "id": "CHK-004",
          "problem_ref": "P04",
          "task": "Correct data types",
          "description": "Each column has the appropriate data type (int, float, datetime, category, bool, object)",
          "checked": false,
          "acceptance_criteria": [
            "print(df.dtypes) confirms correct types",
            "Dates in datetime64[ns]",
            "Numbers in int or float",
            "Categories in category if appropriate"
          ],
          "effort": "Low",
          "impact": "High"
        },
        {
          "id": "CHK-005",
          "problem_ref": "P05",
          "task": "Standardized categories",
          "description": "Categorical values have been normalized (case, accents, spaces, synonyms)",
          "checked": false,
          "acceptance_criteria": [
            "Text in lowercase",
            "Extra spaces removed",
            "Accents removed (if appropriate)",
            "Synonyms mapped"
          ],
          "effort": "Medium",
          "impact": "Medium"
        }
      ]
    },
    {
      "category_id": "QUALITY",
      "category_name": "Data Quality",
      "severity": "CRITICAL",
      "description": "Quality validations and compliance with business rules",
      "order": 2,
      "items": [
        {
          "id": "CHK-006",
          "problem_ref": "P06",
          "task": "Invalid values handled",
          "description": "Values that violate business rules have been validated and handled",
          "checked": false,
          "acceptance_criteria": [
            "Business rules documented (min/max, formats, etc)",
            "Invalid values identified",
            "Obvious correction applied or marked as missing"
          ],
          "effort": "Medium",
          "impact": "Medium"
        },
        {
          "id": "CHK-007",
          "problem_ref": "P07",
          "task": "Features scaled (if necessary for model)",
          "description": "Scales of numeric features have been standardized according to model type",
          "checked": false,
          "acceptance_criteria": [
            "Decision: is scaling necessary? YES/NO documented",
            "Method chosen: StandardScaler, MinMaxScaler, or RobustScaler",
            "Fit on training, applied on test",
            "Scaler saved for production"
          ],
          "effort": "Low",
          "impact": "Medium"
        },
        {
          "id": "CHK-008",
          "problem_ref": "P08",
          "task": "Categorical variables encoded",
          "description": "Categorical variables have been converted to numeric appropriately",
          "checked": false,
          "acceptance_criteria": [
            "Rare categories grouped into 'Other'",
            "Method chosen: OneHotEncoding, Ordinal, or TargetEncoding",
            "Fit on training, applied on test",
            "Drop_first=True to avoid multicollinearity"
          ],
          "effort": "Medium",
          "impact": "High"
        },
        {
          "id": "CHK-009",
          "problem_ref": "P09",
          "task": "Imbalanced classes handled",
          "description": "Unequal distribution of classes has been identified and handled",
          "checked": false,
          "acceptance_criteria": [
            "Imbalance ratio calculated",
            "Decision: resampling, weights, or appropriate metrics?",
            "Method applied documented",
            "Metrics: F1, ROC-AUC (never accuracy alone)"
          ],
          "effort": "Medium",
          "impact": "High"
        },
        {
          "id": "CHK-010",
          "problem_ref": "P10",
          "task": "Bias analyzed and mitigated",
          "description": "Data has been evaluated for sensitive groups and biases identified",
          "checked": false,
          "acceptance_criteria": [
            "Analysis by gender, age, race, location (as applicable)",
            "Statistical test performed (chi-square)",
            "Bias documented: present? YES/NO",
            "Mitigation applied: group balancing, additional data, etc"
          ],
          "effort": "High",
          "impact": "High"
        }
      ]
    },
    {
      "category_id": "FEATURES",
      "category_name": "Feature Engineering & Selection",
      "severity": "HIGH",
      "description": "Optimization of features for model",
      "order": 3,
      "items": [
        {
          "id": "CHK-011",
          "problem_ref": "P11",
          "task": "Irrelevant features removed",
          "description": "Columns with no variance or useless columns have been identified and removed",
          "checked": false,
          "acceptance_criteria": [
            "Constant columns identified",
            "Quasi-constant columns (>99% one value) identified",
            "Features with low correlation to target removed"
          ],
          "effort": "Low",
          "impact": "Medium"
        },
        {
          "id": "CHK-012",
          "problem_ref": "P12",
          "task": "IDs removed",
          "description": "Non-predictive unique identifiers have been removed",
          "checked": false,
          "acceptance_criteria": [
            "Columns id, customer_id, transaction_id, etc identified",
            "Removed from model dataset",
            "Kept in index if traceability needed"
          ],
          "effort": "Low",
          "impact": "Medium"
        },
        {
          "id": "CHK-013",
          "problem_ref": "P13",
          "task": "Multicollinearity handled",
          "description": "Highly correlated features have been identified and handled",
          "checked": false,
          "acceptance_criteria": [
            "Correlation matrix analyzed (>0.95)",
            "VIF calculated (>10 is problematic)",
            "Redundant feature removed or PCA applied"
          ],
          "effort": "Medium",
          "impact": "Medium"
        },
        {
          "id": "CHK-014",
          "problem_ref": "P31",
          "task": "Dimensionality reduced (if necessary)",
          "description": "If many features, dimensionality reduction has been applied",
          "checked": false,
          "acceptance_criteria": [
            "Decision: feature selection or PCA? YES/NO",
            "If YES: method chosen documented",
            "Features reduced from X to Y",
            "Model performance maintained or improved"
          ],
          "effort": "High",
          "impact": "Medium"
        },
        {
          "id": "CHK-015",
          "problem_ref": "P32",
          "task": "Interactions created (if relevant)",
          "description": "Composite features have been created when interactions are identified",
          "checked": false,
          "acceptance_criteria": [
            "Decision: are there relevant interactions? YES/NO",
            "If YES: polynomial features or manual features created",
            "New feature has clear correlation with target"
          ],
          "effort": "Medium",
          "impact": "Low-Medium"
        }
      ]
    },
    {
      "category_id": "TEMPORAL",
      "category_name": "Temporal Data & Leakage",
      "severity": "CRITICAL",
      "description": "Critical to prevent leakage and temporal series issues",
      "order": 4,
      "items": [
        {
          "id": "CHK-016",
          "problem_ref": "P14",
          "task": "Data leakage prevented",
          "description": "No information from the future 'leaks' into model training",
          "checked": false,
          "acceptance_criteria": [
            "Future features identified and removed",
            "Correlation > 0.95 with target investigated",
            "Temporal order validated: feature before target",
            "No feature is consequence of target"
          ],
          "effort": "High",
          "impact": "CRITICAL"
        },
        {
          "id": "CHK-017",
          "problem_ref": "P15",
          "task": "Joins validated",
          "description": "Combinations of multiple sources have been validated",
          "checked": false,
          "acceptance_criteria": [
            "Primary keys unique",
            "Foreign keys valid",
            "Explosion or loss detected and handled",
            "Match rate documented (>95%?)"
          ],
          "effort": "Medium",
          "impact": "High"
        },
        {
          "id": "CHK-018",
          "problem_ref": "P16",
          "task": "Time series ordered",
          "description": "Temporal data has been normalized and correctly ordered",
          "checked": false,
          "acceptance_criteria": [
            "Timezone normalized (UTC or consistent local)",
            "Ordered by timestamp",
            "Frequency uniformized (resample if necessary)",
            "Gaps handled (interpolation or missing)"
          ],
          "effort": "Medium",
          "impact": "High"
        },
        {
          "id": "CHK-019",
          "problem_ref": "P17",
          "task": "Drift detected and handled",
          "description": "Changes in distribution over time have been identified",
          "checked": false,
          "acceptance_criteria": [
            "Distributions compared by period (monthly, quarterly)",
            "KS test performed (p-value < 0.05?)",
            "If drift: recent data prioritized or re-training planned"
          ],
          "effort": "Medium",
          "impact": "High"
        },
        {
          "id": "CHK-020",
          "problem_ref": "P18",
          "task": "Noise reduced",
          "description": "Non-informative noise has been reduced",
          "checked": false,
          "acceptance_criteria": [
            "Decision: is there noise? YES/NO",
            "If YES: method chosen (moving average, Kalman, etc)",
            "Low quality variables removed"
          ],
          "effort": "Medium",
          "impact": "Medium"
        }
      ]
    },
    {
      "category_id": "UNSTRUCTURED",
      "category_name": "Unstructured Data",
      "severity": "HIGH",
      "description": "Handling of text, images, audio",
      "order": 5,
      "items": [
        {
          "id": "CHK-021",
          "problem_ref": "P19",
          "task": "Text processed",
          "description": "Textual data has been cleaned and converted to numeric",
          "checked": false,
          "acceptance_criteria": [
            "Basic cleaning: lowercase, URLs, emails, punctuation removed",
            "Language detected and handled",
            "Vectorization: TF-IDF, Word2Vec, or Transformers applied",
            "If long: chunking applied"
          ],
          "effort": "High",
          "impact": "High"
        },
        {
          "id": "CHK-022",
          "problem_ref": "P20",
          "task": "Images validated",
          "description": "Image files have been validated and standardized",
          "checked": false,
          "acceptance_criteria": [
            "Corrupted files identified and removed",
            "Size standardized (e.g., 224x224)",
            "Pixels normalized ([0,1], [-1,1], or ImageNet)",
            "Augmentation applied only to training"
          ],
          "effort": "Medium",
          "impact": "High"
        },
        {
          "id": "CHK-023",
          "problem_ref": "P21",
          "task": "Audio processed",
          "description": "Audio data has been processed and features extracted",
          "checked": false,
          "acceptance_criteria": [
            "Sample rate standardized (e.g., 16kHz)",
            "Noise reduced",
            "Segmented if necessary",
            "Features extracted: mel-spectrogram, MFCC, ZCR"
          ],
          "effort": "High",
          "impact": "High"
        },
        {
          "id": "CHK-024",
          "problem_ref": "P22",
          "task": "Referential integrity validated",
          "description": "References between tables have been validated",
          "checked": false,
          "acceptance_criteria": [
            "Primary keys are unique",
            "Foreign keys have valid reference",
            "Orphan records identified and handled",
            "Match rate documented"
          ],
          "effort": "Medium",
          "impact": "Medium"
        }
      ]
    },
    {
      "category_id": "ADVANCED",
      "category_name": "Advanced Issues",
      "severity": "MEDIUM",
      "description": "Edge cases and advanced optimizations",
      "order": 6,
      "items": [
        {
          "id": "CHK-025",
          "problem_ref": "P23",
          "task": "Temporal frequencies uniformized",
          "description": "Data with mixed frequencies have been harmonized",
          "checked": false,
          "acceptance_criteria": [
            "Frequencies detected (daily, weekly, monthly?)",
            "Resample or aggregation applied",
            "Final frequency consistent"
          ],
          "effort": "Medium",
          "impact": "Medium"
        },
        {
          "id": "CHK-026",
          "problem_ref": "P24",
          "task": "Temporal gaps handled",
          "description": "Periods without data have been handled",
          "checked": false,
          "acceptance_criteria": [
            "Gaps identified (threshold: days/weeks?)",
            "Decision: interpolate or keep missing?",
            "Method applied documented"
          ],
          "effort": "Medium",
          "impact": "Medium"
        },
        {
          "id": "CHK-027",
          "problem_ref": "P25",
          "task": "Rare categories grouped",
          "description": "Categories with few observations have been grouped",
          "checked": false,
          "acceptance_criteria": [
            "Threshold defined (e.g., <1%)",
            "Rare categories grouped into 'Other' or removed",
            "Final number of categories reduced"
          ],
          "effort": "Low",
          "impact": "Low-Medium"
        },
        {
          "id": "CHK-028",
          "problem_ref": "P26",
          "task": "Constant columns removed",
          "description": "Features with zero variance have been removed",
          "checked": false,
          "acceptance_criteria": [
            "Columns with df[col].nunique() == 1 identified",
            "Removed from dataset"
          ],
          "effort": "Low",
          "impact": "Low"
        },
        {
          "id": "CHK-029",
          "problem_ref": "P27",
          "task": "Target audited",
          "description": "Target variable has been validated for inconsistencies",
          "checked": false,
          "acceptance_criteria": [
            "Class distribution analyzed",
            "Inconsistencies detected (same input, multiple outputs)",
            "Decision: remove, mode, or 'Ambiguous' class?"
          ],
          "effort": "Medium",
          "impact": "CRITICAL"
        }
      ]
    },
    {
      "category_id": "CRITICAL",
      "category_name": "Critical Issues (Essential)",
      "severity": "CRITICAL",
      "description": "MUST-HAVE validations before model",
      "order": 7,
      "items": [
        {
          "id": "CHK-030",
          "problem_ref": "P28",
          "task": "Correct granularity",
          "description": "Unit of analysis is well-defined and consistent",
          "checked": false,
          "acceptance_criteria": [
            "Unit of analysis defined: customer? product? day?",
            "Appropriate aggregation (one row per customer or per transaction?)",
            "Duplication avoided"
          ],
          "effort": "High",
          "impact": "CRITICAL"
        },
        {
          "id": "CHK-031",
          "problem_ref": "P29",
          "task": "Indirect leakage prevented",
          "description": "Features that are consequence of target have been removed",
          "checked": false,
          "acceptance_criteria": [
            "Features identified as consequences removed",
            "Temporal order validated: feature before target",
            "High correlation with target investigated"
          ],
          "effort": "High",
          "impact": "CRITICAL"
        },
        {
          "id": "CHK-032",
          "problem_ref": "P30",
          "task": "Split done correctly",
          "description": "Separation between train/test/validation is correct",
          "checked": false,
          "acceptance_criteria": [
            "Entities not duplicated across partitions",
            "Stratified split if classification",
            "Temporal if time series",
            "Fit ONLY on training, apply on test/validation"
          ],
          "effort": "High",
          "impact": "CRITICAL"
        },
        {
          "id": "CHK-033",
          "problem_ref": "P33",
          "task": "Special values handled",
          "description": "Infinity, very large/small values have been handled",
          "checked": false,
          "acceptance_criteria": [
            "np.inf and -np.inf identified and handled",
            "Values > 1e10 or < -1e10 investigated",
            "Replaced by np.nan or capped"
          ],
          "effort": "Low",
          "impact": "Medium"
        },
        {
          "id": "CHK-034",
          "problem_ref": "P34",
          "task": "Numeric precision validated",
          "description": "Floating-point errors have been mitigated",
          "checked": false,
          "acceptance_criteria": [
            "For money: Decimal used (not float)",
            "Appropriate rounding (e.g., 2 decimal places)",
            "Appropriate scale for very small/large values"
          ],
          "effort": "Low",
          "impact": "Low"
        },
        {
          "id": "CHK-035",
          "problem_ref": "P35",
          "task": "CV appropriate to data type",
          "description": "Cross-validation has been configured correctly",
          "checked": false,
          "acceptance_criteria": [
            "Random data: StratifiedKFold",
            "Time series: TimeSeriesSplit",
            "Hierarchical data: GroupKFold",
            "Never random on time series"
          ],
          "effort": "Medium",
          "impact": "High"
        }
      ]
    }
  ],
  
  "summary": {
    "total_items": 88,
    "critical_items": 24,
    "high_items": 19,
    "medium_items": 28,
    "low_items": 17,
    "completion_checklist": [
      {
        "stage": "1. Basic Preparation",
        "items_count": 5,
        "estimated_days": "1-2",
        "critical": true,
        "description": "Verify fundamental data structure"
      },
      {
        "stage": "2. Data Quality",
        "items_count": 5,
        "estimated_days": "2-3",
        "critical": true,
        "description": "Validate compliance and business rules"
      },
      {
        "stage": "3. Feature Engineering",
        "items_count": 5,
        "estimated_days": "3-5",
        "critical": false,
        "description": "Optimize features for model"
      },
      {
        "stage": "4. Temporal Data & Leakage",
        "items_count": 5,
        "estimated_days": "2-4",
        "critical": true,
        "description": "Prevent leakage and temporal issues"
      },
      {
        "stage": "5. Unstructured Data",
        "items_count": 4,
        "estimated_days": "3-7",
        "critical": false,
        "description": "Process text, images, audio"
      },
      {
        "stage": "6. Advanced Issues",
        "items_count": 5,
        "estimated_days": "2-3",
        "critical": false,
        "description": "Edge cases and optimizations"
      },
      {
        "stage": "7. Critical Issues",
        "items_count": 6,
        "estimated_days": "2-3",
        "critical": true,
        "description": "MUST-HAVE validations before model"
      }
    ]
  },
  
  "quick_validation": {
    "must_pass": [
      "CHK-001 - Missing values handled",
      "CHK-004 - Correct data types",
      "CHK-016 - Data leakage prevented",
      "CHK-017 - Joins validated",
      "CHK-030 - Correct granularity",
      "CHK-031 - Indirect leakage prevented",
      "CHK-032 - Split done correctly"
    ],
    "should_pass": [
      "CHK-002 - Duplicates removed",
      "CHK-005 - Standardized categories",
      "CHK-008 - Categorical variables encoded",
      "CHK-009 - Imbalanced classes handled",
      "CHK-018 - Time series ordered"
    ],
    "nice_to_have": [
      "CHK-014 - Dimensionality reduced",
      "CHK-015 - Interactions created",
      "CHK-019 - Drift detected"
    ]
  },

  "validation_script_template": {
    "description": "Python template to automatically validate checklist",
    "code": "import pandas as pd\nimport numpy as np\n\nclass DataPreparationValidator:\n    def __init__(self, df):\n        self.df = df\n        self.results = {}\n    \n    def check_missing_values(self):\n        missing_rate = (self.df.isnull().sum() / len(self.df) * 100).to_dict()\n        self.results['CHK-001'] = {\n            'passed': self.df.isnull().sum().sum() == 0,\n            'missing_by_column': missing_rate\n        }\n    \n    def check_duplicates(self):\n        duplicates = self.df.duplicated().sum()\n        self.results['CHK-002'] = {\n            'passed': duplicates == 0,\n            'duplicates_found': duplicates\n        }\n    \n    def check_data_types(self):\n        self.results['CHK-004'] = {\n            'passed': True,\n            'data_types': self.df.dtypes.to_dict()\n        }\n    \n    def check_no_leakage(self, features, target):\n        # High correlation check\n        correlations = self.df[features].corrwith(self.df[target]).abs()\n        suspicious = correlations[correlations > 0.95]\n        self.results['CHK-016'] = {\n            'passed': len(suspicious) == 0,\n            'suspicious_features': suspicious.to_dict() if len(suspicious) > 0 else {}\n        }\n    \n    def run_all_checks(self):\n        self.check_missing_values()\n        self.check_duplicates()\n        self.check_data_types()\n        return self.results\n    \n    def print_report(self):\n        print('='*50)\n        print('DATA PREPARATION VALIDATION REPORT')\n        print('='*50)\n        for check_id, result in self.results.items():\n            status = '✓ PASS' if result['passed'] else '✗ FAIL'\n            print(f\"{check_id}: {status}\")\n        print('='*50)\n\n# Usage:\n# validator = DataPreparationValidator(df)\n# validator.run_all_checks()\n# validator.print_report()\n"
  },

  "notes": {
    "how_to_use": "Check each item as complete when validated",
    "color_coding": {
      "red": "CRITICAL - MUST PASS",
      "yellow": "HIGH - STRONGLY RECOMMENDED",
      "green": "MEDIUM/LOW - NICE TO HAVE"
    },
    "effort_estimation": "Estimates are per item, total time ~2-4 weeks depending on dataset size",
    "documentation": "Document method and reason for each decision taken"
  }
}